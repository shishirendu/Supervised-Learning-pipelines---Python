{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Scikit-Learn Version: ', '0.20.4')\n",
      "('Pandas Version: ', u'0.23.4')\n",
      "('Numpy Version: ', '1.16.6')\n",
      "('Seaborn Version: ', '0.9.0')\n",
      "('LightGBM Version: ', '2.3.1')\n"
     ]
    }
   ],
   "source": [
    "#import warnings\n",
    "#warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn import datasets\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_auc_score, f1_score, log_loss, average_precision_score \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import truncnorm, uniform as sp_randFloat, uniform\n",
    "from scipy.stats import randint as sp_randInt\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Scikit-Learn Version: \", sklearn.__version__)\n",
    "print(\"Pandas Version: \", pd.__version__)\n",
    "print(\"Numpy Version: \", np.__version__)\n",
    "print(\"Seaborn Version: \", sns.__version__)\n",
    "print(\"LightGBM Version: \", lgb.__version__)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining constants\n",
    "\n",
    "filename = 'Extract with ncfas9.csv'\n",
    "kfold = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-5-1c19d771e2ec>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-1c19d771e2ec>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    dataset = pd.read_csv(filename)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#def load_dataset(filename):\n",
    "    \n",
    "    dataset = pd.read_csv(filename)\n",
    "    \n",
    "    print(dataset.shape)\n",
    "    print(dataset.head(5))\n",
    "    print(dataset.columns)\n",
    "    print(dataset.dtypes)\n",
    "    \n",
    "    feature_names = ['IsSuccessorFailure', 'Week0', 'Week1', 'Week2', 'Week3', 'Week4',\n",
    "       'Week5', 'Week6', 'Week7', 'Week8', 'fldEnvironment1',\n",
    "       'fldEnvironment2', 'fldEnvironment3', 'fldEnvironment4',\n",
    "       'fldEnvironment5', 'fldEnvironment6', 'fldEnvironment7',\n",
    "       'fldParenting1', 'fldParenting2', 'fldParenting3', 'fldParenting4',\n",
    "       'fldParenting5', 'fldParenting6', 'fldParenting7', 'fldParenting8',\n",
    "       'fldFamily1', 'fldFamily2', 'fldFamily3', 'fldFamily4',\n",
    "       'fldFamily5', 'fldFamily6', 'fldFamily7', 'fldFamily8',\n",
    "       'fldSafety1', 'fldSafety2', 'fldSafety3', 'fldSafety4',\n",
    "       'fldSafety5', 'fldSafety6', 'fldSafety7', 'fldSafety8',\n",
    "       'fldWellbeing1', 'fldWellbeing2', 'fldWellbeing3', 'fldWellbeing4',\n",
    "       'fldWellbeing5', 'fldWellbeing6', 'fldWellbeing7', 'fldSocial1',\n",
    "       'fldSocial2', 'fldSocial3', 'fldSocial4', 'fldSocial5',\n",
    "       'fldSocial6', 'fldSelfSufficiency1', 'fldSelfSufficiency2',\n",
    "       'fldSelfSufficiency3', 'fldSelfSufficiency4', 'fldSelfSufficiency5',\n",
    "       'fldSelfSufficiency6', 'fldHealth1', 'fldHealth2', 'fldHealth3',\n",
    "       'fldHealth4', 'fldHealth5', 'fldHealth6', 'fldHealth7',\n",
    "       'fldHealth8', 'fldAmbivalence1', 'fldAmbivalence2',\n",
    "       'fldAmbivalence3', 'fldAmbivalence4', 'fldAmbivalence5',\n",
    "       'fldAmbivalence6', 'fldReadiness1', 'fldReadiness2',\n",
    "       'fldReadiness3', 'fldReadiness4', 'fldReadiness5', 'fldReadiness6']\n",
    "    \n",
    "    target = 'IsSuccessorFailure'\n",
    "    \n",
    "    return feature_names, target, dataset\n",
    "\n",
    "#Execute the function\n",
    "\n",
    "load_dataset(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(503, 80)\n",
      "  IsSuccessorFailure  Week0  Week1  Week2  Week3  Week4  Week5  Week6  Week7  \\\n",
      "0            Success      1      1      3      3      1      2      0      3   \n",
      "1            Success      1      2      2      2      0      6      5      5   \n",
      "2            Success      1      0      1      1      2      1      1      1   \n",
      "3            Failure     10      3      3      5      3      0      2      4   \n",
      "4            Success      1      0      1      1      2      1      1      1   \n",
      "\n",
      "   Week8      ...        fldAmbivalence3  fldAmbivalence4  fldAmbivalence5  \\\n",
      "0      0      ...                      0                0                0   \n",
      "1      1      ...                      6                6                6   \n",
      "2      0      ...                      0                0                0   \n",
      "3      0      ...                     -1               -1               -1   \n",
      "4      1      ...                      0                0                0   \n",
      "\n",
      "   fldAmbivalence6  fldReadiness1  fldReadiness2  fldReadiness3  \\\n",
      "0                0              0              0              0   \n",
      "1                6              6              6              6   \n",
      "2                0              0              0              0   \n",
      "3               -1              4              4              3   \n",
      "4                0              0              0              0   \n",
      "\n",
      "   fldReadiness4  fldReadiness5  fldReadiness6  \n",
      "0              0              0              0  \n",
      "1              6              6              6  \n",
      "2              0              0              0  \n",
      "3              3              4              3  \n",
      "4              0              0              0  \n",
      "\n",
      "[5 rows x 80 columns]\n",
      "Index([u'IsSuccessorFailure', u'Week0', u'Week1', u'Week2', u'Week3', u'Week4',\n",
      "       u'Week5', u'Week6', u'Week7', u'Week8', u'fldEnvironment1',\n",
      "       u'fldEnvironment2', u'fldEnvironment3', u'fldEnvironment4',\n",
      "       u'fldEnvironment5', u'fldEnvironment6', u'fldEnvironment7',\n",
      "       u'fldParenting1', u'fldParenting2', u'fldParenting3', u'fldParenting4',\n",
      "       u'fldParenting5', u'fldParenting6', u'fldParenting7', u'fldParenting8',\n",
      "       u'fldFamily1', u'fldFamily2', u'fldFamily3', u'fldFamily4',\n",
      "       u'fldFamily5', u'fldFamily6', u'fldFamily7', u'fldFamily8',\n",
      "       u'fldSafety1', u'fldSafety2', u'fldSafety3', u'fldSafety4',\n",
      "       u'fldSafety5', u'fldSafety6', u'fldSafety7', u'fldSafety8',\n",
      "       u'fldWellbeing1', u'fldWellbeing2', u'fldWellbeing3', u'fldWellbeing4',\n",
      "       u'fldWellbeing5', u'fldWellbeing6', u'fldWellbeing7', u'fldSocial1',\n",
      "       u'fldSocial2', u'fldSocial3', u'fldSocial4', u'fldSocial5',\n",
      "       u'fldSocial6', u'fldSelfSufficiency1', u'fldSelfSufficiency2',\n",
      "       u'fldSelfSufficiency3', u'fldSelfSufficiency4', u'fldSelfSufficiency5',\n",
      "       u'fldSelfSufficiency6', u'fldHealth1', u'fldHealth2', u'fldHealth3',\n",
      "       u'fldHealth4', u'fldHealth5', u'fldHealth6', u'fldHealth7',\n",
      "       u'fldHealth8', u'fldAmbivalence1', u'fldAmbivalence2',\n",
      "       u'fldAmbivalence3', u'fldAmbivalence4', u'fldAmbivalence5',\n",
      "       u'fldAmbivalence6', u'fldReadiness1', u'fldReadiness2',\n",
      "       u'fldReadiness3', u'fldReadiness4', u'fldReadiness5', u'fldReadiness6'],\n",
      "      dtype='object')\n",
      "IsSuccessorFailure     object\n",
      "Week0                   int64\n",
      "Week1                   int64\n",
      "Week2                   int64\n",
      "Week3                   int64\n",
      "Week4                   int64\n",
      "Week5                   int64\n",
      "Week6                   int64\n",
      "Week7                   int64\n",
      "Week8                   int64\n",
      "fldEnvironment1         int64\n",
      "fldEnvironment2         int64\n",
      "fldEnvironment3         int64\n",
      "fldEnvironment4         int64\n",
      "fldEnvironment5         int64\n",
      "fldEnvironment6         int64\n",
      "fldEnvironment7         int64\n",
      "fldParenting1           int64\n",
      "fldParenting2           int64\n",
      "fldParenting3           int64\n",
      "fldParenting4           int64\n",
      "fldParenting5           int64\n",
      "fldParenting6           int64\n",
      "fldParenting7           int64\n",
      "fldParenting8           int64\n",
      "fldFamily1              int64\n",
      "fldFamily2              int64\n",
      "fldFamily3              int64\n",
      "fldFamily4              int64\n",
      "fldFamily5              int64\n",
      "                        ...  \n",
      "fldSocial3              int64\n",
      "fldSocial4              int64\n",
      "fldSocial5              int64\n",
      "fldSocial6              int64\n",
      "fldSelfSufficiency1     int64\n",
      "fldSelfSufficiency2     int64\n",
      "fldSelfSufficiency3     int64\n",
      "fldSelfSufficiency4     int64\n",
      "fldSelfSufficiency5     int64\n",
      "fldSelfSufficiency6     int64\n",
      "fldHealth1              int64\n",
      "fldHealth2              int64\n",
      "fldHealth3              int64\n",
      "fldHealth4              int64\n",
      "fldHealth5              int64\n",
      "fldHealth6              int64\n",
      "fldHealth7              int64\n",
      "fldHealth8              int64\n",
      "fldAmbivalence1         int64\n",
      "fldAmbivalence2         int64\n",
      "fldAmbivalence3         int64\n",
      "fldAmbivalence4         int64\n",
      "fldAmbivalence5         int64\n",
      "fldAmbivalence6         int64\n",
      "fldReadiness1           int64\n",
      "fldReadiness2           int64\n",
      "fldReadiness3           int64\n",
      "fldReadiness4           int64\n",
      "fldReadiness5           int64\n",
      "fldReadiness6           int64\n",
      "Length: 80, dtype: object\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(filename)\n",
    "    \n",
    "print(dataset.shape)\n",
    "print(dataset.head(5))\n",
    "print(dataset.columns)\n",
    "print(dataset.dtypes)\n",
    "    \n",
    "dataset['trgt'] = pd.factorize(dataset['IsSuccessorFailure'])[0]\n",
    "    \n",
    "target = 'trgt'\n",
    "    \n",
    "feature_names = ['Week1', 'Week2', 'Week7', 'Week8', 'fldEnvironment1', 'fldEnvironment2', 'fldParenting1', 'fldParenting2',\n",
    "                     'fldParenting4', 'fldParenting5', 'fldParenting6', 'fldParenting7', 'fldParenting8', 'fldFamily1',\n",
    "                     'fldFamily3', 'fldSafety1', 'fldSafety2', 'fldSafety6', 'fldSafety7', 'fldSafety8', 'fldWellbeing4',\n",
    "                     'fldSocial2', 'fldSocial5', 'fldSocial6', 'fldHealth1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(503, 80)\n",
      "  IsSuccessorFailure  Week0  Week1  Week2  Week3  Week4  Week5  Week6  Week7  \\\n",
      "0            Success      1      1      3      3      1      2      0      3   \n",
      "1            Success      1      2      2      2      0      6      5      5   \n",
      "2            Success      1      0      1      1      2      1      1      1   \n",
      "3            Failure     10      3      3      5      3      0      2      4   \n",
      "4            Success      1      0      1      1      2      1      1      1   \n",
      "\n",
      "   Week8      ...        fldAmbivalence3  fldAmbivalence4  fldAmbivalence5  \\\n",
      "0      0      ...                      0                0                0   \n",
      "1      1      ...                      6                6                6   \n",
      "2      0      ...                      0                0                0   \n",
      "3      0      ...                     -1               -1               -1   \n",
      "4      1      ...                      0                0                0   \n",
      "\n",
      "   fldAmbivalence6  fldReadiness1  fldReadiness2  fldReadiness3  \\\n",
      "0                0              0              0              0   \n",
      "1                6              6              6              6   \n",
      "2                0              0              0              0   \n",
      "3               -1              4              4              3   \n",
      "4                0              0              0              0   \n",
      "\n",
      "   fldReadiness4  fldReadiness5  fldReadiness6  \n",
      "0              0              0              0  \n",
      "1              6              6              6  \n",
      "2              0              0              0  \n",
      "3              3              4              3  \n",
      "4              0              0              0  \n",
      "\n",
      "[5 rows x 80 columns]\n",
      "Index([u'IsSuccessorFailure', u'Week0', u'Week1', u'Week2', u'Week3', u'Week4',\n",
      "       u'Week5', u'Week6', u'Week7', u'Week8', u'fldEnvironment1',\n",
      "       u'fldEnvironment2', u'fldEnvironment3', u'fldEnvironment4',\n",
      "       u'fldEnvironment5', u'fldEnvironment6', u'fldEnvironment7',\n",
      "       u'fldParenting1', u'fldParenting2', u'fldParenting3', u'fldParenting4',\n",
      "       u'fldParenting5', u'fldParenting6', u'fldParenting7', u'fldParenting8',\n",
      "       u'fldFamily1', u'fldFamily2', u'fldFamily3', u'fldFamily4',\n",
      "       u'fldFamily5', u'fldFamily6', u'fldFamily7', u'fldFamily8',\n",
      "       u'fldSafety1', u'fldSafety2', u'fldSafety3', u'fldSafety4',\n",
      "       u'fldSafety5', u'fldSafety6', u'fldSafety7', u'fldSafety8',\n",
      "       u'fldWellbeing1', u'fldWellbeing2', u'fldWellbeing3', u'fldWellbeing4',\n",
      "       u'fldWellbeing5', u'fldWellbeing6', u'fldWellbeing7', u'fldSocial1',\n",
      "       u'fldSocial2', u'fldSocial3', u'fldSocial4', u'fldSocial5',\n",
      "       u'fldSocial6', u'fldSelfSufficiency1', u'fldSelfSufficiency2',\n",
      "       u'fldSelfSufficiency3', u'fldSelfSufficiency4', u'fldSelfSufficiency5',\n",
      "       u'fldSelfSufficiency6', u'fldHealth1', u'fldHealth2', u'fldHealth3',\n",
      "       u'fldHealth4', u'fldHealth5', u'fldHealth6', u'fldHealth7',\n",
      "       u'fldHealth8', u'fldAmbivalence1', u'fldAmbivalence2',\n",
      "       u'fldAmbivalence3', u'fldAmbivalence4', u'fldAmbivalence5',\n",
      "       u'fldAmbivalence6', u'fldReadiness1', u'fldReadiness2',\n",
      "       u'fldReadiness3', u'fldReadiness4', u'fldReadiness5', u'fldReadiness6'],\n",
      "      dtype='object')\n",
      "IsSuccessorFailure     object\n",
      "Week0                   int64\n",
      "Week1                   int64\n",
      "Week2                   int64\n",
      "Week3                   int64\n",
      "Week4                   int64\n",
      "Week5                   int64\n",
      "Week6                   int64\n",
      "Week7                   int64\n",
      "Week8                   int64\n",
      "fldEnvironment1         int64\n",
      "fldEnvironment2         int64\n",
      "fldEnvironment3         int64\n",
      "fldEnvironment4         int64\n",
      "fldEnvironment5         int64\n",
      "fldEnvironment6         int64\n",
      "fldEnvironment7         int64\n",
      "fldParenting1           int64\n",
      "fldParenting2           int64\n",
      "fldParenting3           int64\n",
      "fldParenting4           int64\n",
      "fldParenting5           int64\n",
      "fldParenting6           int64\n",
      "fldParenting7           int64\n",
      "fldParenting8           int64\n",
      "fldFamily1              int64\n",
      "fldFamily2              int64\n",
      "fldFamily3              int64\n",
      "fldFamily4              int64\n",
      "fldFamily5              int64\n",
      "                        ...  \n",
      "fldSocial3              int64\n",
      "fldSocial4              int64\n",
      "fldSocial5              int64\n",
      "fldSocial6              int64\n",
      "fldSelfSufficiency1     int64\n",
      "fldSelfSufficiency2     int64\n",
      "fldSelfSufficiency3     int64\n",
      "fldSelfSufficiency4     int64\n",
      "fldSelfSufficiency5     int64\n",
      "fldSelfSufficiency6     int64\n",
      "fldHealth1              int64\n",
      "fldHealth2              int64\n",
      "fldHealth3              int64\n",
      "fldHealth4              int64\n",
      "fldHealth5              int64\n",
      "fldHealth6              int64\n",
      "fldHealth7              int64\n",
      "fldHealth8              int64\n",
      "fldAmbivalence1         int64\n",
      "fldAmbivalence2         int64\n",
      "fldAmbivalence3         int64\n",
      "fldAmbivalence4         int64\n",
      "fldAmbivalence5         int64\n",
      "fldAmbivalence6         int64\n",
      "fldReadiness1           int64\n",
      "fldReadiness2           int64\n",
      "fldReadiness3           int64\n",
      "fldReadiness4           int64\n",
      "fldReadiness5           int64\n",
      "fldReadiness6           int64\n",
      "Length: 80, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#def load_dataset(filename):\n",
    "    \n",
    "dataset = pd.read_csv(filename)\n",
    "   \n",
    "print(dataset.shape)\n",
    "print(dataset.head(5))\n",
    "print(dataset.columns)\n",
    "print(dataset.dtypes)\n",
    "\n",
    "dataset['trgt'] = pd.factorize(dataset['IsSuccessorFailure'])[0]\n",
    "    \n",
    "target = 'trgt'\n",
    "    \n",
    "feature_names = ['Week1', 'Week2', 'Week7', 'Week8', 'fldEnvironment1', 'fldEnvironment2', 'fldParenting1', 'fldParenting2',\n",
    "                     'fldParenting4', 'fldParenting5', 'fldParenting6', 'fldParenting7', 'fldParenting8', 'fldFamily1',\n",
    "                     'fldFamily3', 'fldSafety1', 'fldSafety2', 'fldSafety6', 'fldSafety7', 'fldSafety8', 'fldWellbeing4',\n",
    "                     'fldSocial2', 'fldSocial5', 'fldSocial6', 'fldHealth1']\n",
    "    \n",
    "    #target = 'IsSuccessorFailure'\n",
    "    \n",
    " #   return feature_names, target, dataset\n",
    "\n",
    "#Execute the function\n",
    "\n",
    "#load_dataset(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(402, 25)\n",
      "(402L,)\n",
      "(101, 25)\n",
      "(101L,)\n"
     ]
    }
   ],
   "source": [
    "def data_split(feature_names, target, dataset):\n",
    "    #split into train and test datasets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset.loc[:, feature_names], \n",
    "                                                        dataset.loc[:, target], test_size = 0.20)\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_test.shape)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = data_split(feature_names, target, dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(X_train, y_train):\n",
    "    model = lgb.LGBMClassifier()\n",
    "    #Grid Search\n",
    "    \n",
    "    \n",
    "    parameters = {'max_depth'     : range(4, 10),\n",
    "                  'learning_rate' : (0.01, 0.05, 0.1),\n",
    "                  'num_iteration' : (2000, 2500, 3000),\n",
    "                  'n_estimators'  : (100, 150, 200),\n",
    "                  'max_features'  : (10, 12, 15, 18, 20),\n",
    "                  'num_leaves'    : (50, 100)\n",
    "         #         'max_bins'      : (5, 6, 7),\n",
    "          #        'min_child_samples': (3, 5, 8, 10, 12),\n",
    "    }\n",
    "    grid = GridSearchCV(estimator = model, param_grid = parameters, \n",
    "                       cv = kfold, verbose = 1, n_jobs = -1, refit = True)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    #Results from Grid Search\n",
    "    print(\"\\n==============================================\")\n",
    "    print(\"The best Parameter: \", grid.best_estimator_)\n",
    "    print(\"The best parameter :\", grid.best_params_)\n",
    "    \n",
    "    return grid.best_estimator_\n",
    "\n",
    "model = training_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.1s remaining:    0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cv_results', array([0.7654321 , 0.79012346, 0.80246914, 0.8       , 0.87341772]))\n",
      "()\n",
      "{'num_leaves': 31, 'reg_alpha': 0.0, 'n_jobs': -1, 'learning_rate': 0.05, 'n_estimators': 100, 'num_iteration': 50, 'subsample_for_bin': 200000, 'max_features': 10, 'importance_type': 'split', 'boosting_type': 'gbdt', 'colsample_bytree': 1.0, 'silent': True, 'subsample_freq': 0, 'min_child_samples': 10, 'max_bins': 5, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'subsample': 1.0, 'reg_lambda': 0.0, 'random_state': None, 'objective': None, 'class_weight': None, 'max_depth': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation using the best fit model\n",
    "\n",
    "\n",
    "\n",
    "def cross_validation_and_fitting(model, X_train, y_train):\n",
    " #   model = training_model(X_train, y_train)\n",
    "        \n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv = kfold, scoring = 'accuracy', n_jobs = -1, verbose = 1)\n",
    "    print('cv_results', cv_results)\n",
    "   \n",
    "    #Final fitting of the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    print(); print(model.get_params(deep = True))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = cross_validation_and_fitting(model, X_train, y_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy of the model is :', 0.7326732673267327)\n",
      "('Balanced Accuracy of the model is :', 0.6065613026819924)\n",
      "('Area under curve and ROC :', 0.6065613026819923)\n",
      "('Confusion Matrix :', array([[65,  7],\n",
      "       [20,  9]], dtype=int64))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.90      0.83        72\n",
      "           1       0.56      0.31      0.40        29\n",
      "\n",
      "   micro avg       0.73      0.73      0.73       101\n",
      "   macro avg       0.66      0.61      0.61       101\n",
      "weighted avg       0.71      0.73      0.71       101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained model on test dataset\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    balancedaccuracy = balanced_accuracy_score(y_test, pred)\n",
    "    rocauc = roc_auc_score(y_test, pred)\n",
    "    confmat = confusion_matrix(y_test, pred)\n",
    "    classrprt = classification_report(y_test, pred)\n",
    "    \n",
    "    print(\"Accuracy of the model is :\", accuracy)\n",
    "    print(\"Balanced Accuracy of the model is :\", balancedaccuracy)\n",
    "    print(\"Area under curve and ROC :\", rocauc)\n",
    "  #  print(\"F1 Score of the model is :\", fonescore)\n",
    "    print(\"Confusion Matrix :\", confmat)\n",
    "    print(classrprt)\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "model = evaluate_model(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the following model for Randomized Search optimized with various hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   13.4s finished\n",
      "C:\\Users\\sjha\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:842: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\sjha\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iteration` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best estimators', LGBMClassifier(boosting_type='gbdt', bootstrap='True', class_weight=None,\n",
      "        colsample_bytree=1.0, importance_type='split',\n",
      "        learning_rate=0.8054942284869145, max_depth=9, max_features='sqrt',\n",
      "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "        n_estimators=100, n_jobs=-1, num_iteration=3078, num_leaves=57,\n",
      "        objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "        silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "        subsample_freq=0))\n",
      "('Best Score', 0.736318407960199)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def training_model(X_train, y_train):\n",
    "    model = lgb.LGBMClassifier()\n",
    "    #Grid Search\n",
    "\n",
    "    \n",
    "    parameters = {'max_depth'     : sp_randInt(6, 12),\n",
    "                \n",
    "                  'learning_rate'  : sp_randFloat(0.01, 0.99),\n",
    "                  'num_iteration'  : sp_randInt(10, 5000),\n",
    "              #    'n_estimator'    : sp_randInt(10, 5000),\n",
    "                  'num_leaves'     : sp_randInt(50, 100),\n",
    "                  'max_features'   : ['auto', 'sqrt'],\n",
    "                  'bootstrap'      : ['True', 'False']\n",
    "                  #'min_sample_split': uniform(0.01, 0.199)\n",
    "         \n",
    "    }\n",
    "    \n",
    "    randm = RandomizedSearchCV(estimator = model, param_distributions = parameters, cv = kfold, n_iter = 10, verbose = 1, n_jobs = -1)\n",
    "    randm.fit(X_train, y_train)\n",
    "    \n",
    "    #Results from random search\n",
    "    print(\"Best estimators\", randm.best_estimator_)\n",
    "    print(\"Best Score\", randm.best_score_)\n",
    "    \n",
    "    return randm.best_estimator_\n",
    "    \n",
    "#    return grid.best_estimator_    \n",
    "\n",
    "model = training_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.6s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cv_results', array([0.7037037 , 0.7654321 , 0.81481481, 0.8       , 0.79746835]))\n",
      "()\n",
      "{'max_depths': 10, 'num_leaves': 31, 'reg_alpha': 0.0, 'n_jobs': -1, 'learning_rate': 0.459014334833276, 'n_estimators': 100, 'num_iteration': 1640, 'subsample_for_bin': 200000, 'n_estimator': 1053, 'importance_type': 'split', 'boosting_type': 'gbdt', 'colsample_bytree': 1.0, 'silent': True, 'subsample_freq': 0, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'subsample': 1.0, 'reg_lambda': 0.0, 'random_state': None, 'objective': None, 'class_weight': None, 'max_depth': -1}\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation using the best fit model\n",
    "\n",
    "\n",
    "\n",
    "def cross_validation_and_fitting(model, X_train, y_train):\n",
    " #   model = training_model(X_train, y_train)\n",
    "        \n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv = 5, scoring = 'accuracy', n_jobs = -1, verbose = 1)\n",
    "    print('cv_results', cv_results)\n",
    "   \n",
    "    #Final fitting of the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    print(); print(model.get_params(deep = True))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = cross_validation_and_fitting(model, X_train, y_train)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy of the model is :', 0.8316831683168316)\n",
      "('Balanced Accuracy of the model is :', 0.8179112554112553)\n",
      "('Area under curve and ROC :', 0.8179112554112553)\n",
      "('Confusion Matrix :', array([[65, 12],\n",
      "       [ 5, 19]], dtype=int64))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.84      0.88        77\n",
      "           1       0.61      0.79      0.69        24\n",
      "\n",
      "   micro avg       0.83      0.83      0.83       101\n",
      "   macro avg       0.77      0.82      0.79       101\n",
      "weighted avg       0.85      0.83      0.84       101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained model on test dataset\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    balancedaccuracy = balanced_accuracy_score(y_test, pred)\n",
    "    rocauc = roc_auc_score(y_test, pred)\n",
    "    confmat = confusion_matrix(y_test, pred)\n",
    "    classrprt = classification_report(y_test, pred)\n",
    "    \n",
    "    print(\"Accuracy of the model is :\", accuracy)\n",
    "    print(\"Balanced Accuracy of the model is :\", balancedaccuracy)\n",
    "    print(\"Area under curve and ROC :\", rocauc)\n",
    "  #  print(\"F1 Score of the model is :\", fonescore)\n",
    "    print(\"Confusion Matrix :\", confmat)\n",
    "    print(classrprt)\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "model = evaluate_model(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Does not give a good model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "#                       Model Training\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "def training_model(X_train, y_train):\n",
    "    model = lgb.LGBMClassifier()\n",
    "    #Grid Search\n",
    "    parameters = {'max_depth'     : range(4, 10),\n",
    "                  'learning_rate' : (0.3, 0.6, 0.7, 0.8, 0.9),\n",
    "                  'num_iteration' : (10, 15, 20, 25, 30, 35, 40, 45, 50), #(2000, 2500, 3000, 3500, 4000),\n",
    "                  'n_estimators'  : (10, 15, 20, 25, 30, 35, 40, 45, 50),\n",
    "                  'max_features'  : (10, 12, 15, 18, 20),\n",
    "                  'max_bins'      : (5, 6, 7),\n",
    "                  'min_child_samples': (3, 5, 8, 10, 12),\n",
    "    }\n",
    "    grid = GridSearchCV(estimator = model, param_grid = parameters, \n",
    "                       cv = kfold, verbose = 1, n_jobs = -1, refit = True)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    #Results from Grid Search\n",
    "    print(\"\\n==============================================\")\n",
    "    print(\"The best Parameter: \", grid.best_estimator_)\n",
    "    print(\"The best parameter :\", grid.best_params_)\n",
    "    \n",
    "    \n",
    "    return grid.best_estimator_    \n",
    "\n",
    "#model = training_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 182250 candidates, totalling 911250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   38.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   41.3s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   47.0s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   55.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4984 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6034 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 7184 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 8434 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 9784 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 11234 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 12784 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 14434 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 16184 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=-1)]: Done 18034 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=-1)]: Done 19984 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=-1)]: Done 22034 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=-1)]: Done 24184 tasks      | elapsed: 11.5min\n",
      "[Parallel(n_jobs=-1)]: Done 26434 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=-1)]: Done 28784 tasks      | elapsed: 13.6min\n",
      "[Parallel(n_jobs=-1)]: Done 31234 tasks      | elapsed: 14.6min\n",
      "[Parallel(n_jobs=-1)]: Done 33784 tasks      | elapsed: 15.7min\n",
      "[Parallel(n_jobs=-1)]: Done 36434 tasks      | elapsed: 16.7min\n",
      "[Parallel(n_jobs=-1)]: Done 39184 tasks      | elapsed: 17.8min\n",
      "[Parallel(n_jobs=-1)]: Done 42034 tasks      | elapsed: 19.1min\n",
      "[Parallel(n_jobs=-1)]: Done 44984 tasks      | elapsed: 20.6min\n",
      "[Parallel(n_jobs=-1)]: Done 48034 tasks      | elapsed: 22.0min\n",
      "[Parallel(n_jobs=-1)]: Done 51184 tasks      | elapsed: 23.6min\n",
      "[Parallel(n_jobs=-1)]: Done 54434 tasks      | elapsed: 25.3min\n",
      "[Parallel(n_jobs=-1)]: Done 57784 tasks      | elapsed: 26.9min\n",
      "[Parallel(n_jobs=-1)]: Done 61234 tasks      | elapsed: 28.3min\n",
      "[Parallel(n_jobs=-1)]: Done 64784 tasks      | elapsed: 29.7min\n",
      "[Parallel(n_jobs=-1)]: Done 68434 tasks      | elapsed: 31.1min\n",
      "[Parallel(n_jobs=-1)]: Done 72184 tasks      | elapsed: 32.5min\n",
      "[Parallel(n_jobs=-1)]: Done 76034 tasks      | elapsed: 34.0min\n",
      "[Parallel(n_jobs=-1)]: Done 79984 tasks      | elapsed: 35.7min\n",
      "[Parallel(n_jobs=-1)]: Done 84034 tasks      | elapsed: 37.7min\n",
      "[Parallel(n_jobs=-1)]: Done 88184 tasks      | elapsed: 39.5min\n",
      "[Parallel(n_jobs=-1)]: Done 92434 tasks      | elapsed: 41.2min\n",
      "[Parallel(n_jobs=-1)]: Done 96784 tasks      | elapsed: 42.9min\n",
      "[Parallel(n_jobs=-1)]: Done 101234 tasks      | elapsed: 44.6min\n",
      "[Parallel(n_jobs=-1)]: Done 105784 tasks      | elapsed: 46.6min\n",
      "[Parallel(n_jobs=-1)]: Done 110434 tasks      | elapsed: 48.6min\n",
      "[Parallel(n_jobs=-1)]: Done 115184 tasks      | elapsed: 50.6min\n",
      "[Parallel(n_jobs=-1)]: Done 120034 tasks      | elapsed: 53.8min\n",
      "[Parallel(n_jobs=-1)]: Done 124984 tasks      | elapsed: 56.8min\n",
      "[Parallel(n_jobs=-1)]: Done 130034 tasks      | elapsed: 59.5min\n",
      "[Parallel(n_jobs=-1)]: Done 135184 tasks      | elapsed: 62.5min\n",
      "[Parallel(n_jobs=-1)]: Done 140434 tasks      | elapsed: 65.0min\n",
      "[Parallel(n_jobs=-1)]: Done 145784 tasks      | elapsed: 67.8min\n",
      "[Parallel(n_jobs=-1)]: Done 151234 tasks      | elapsed: 70.9min\n",
      "[Parallel(n_jobs=-1)]: Done 156784 tasks      | elapsed: 73.7min\n",
      "[Parallel(n_jobs=-1)]: Done 162434 tasks      | elapsed: 76.6min\n",
      "[Parallel(n_jobs=-1)]: Done 168184 tasks      | elapsed: 79.7min\n",
      "[Parallel(n_jobs=-1)]: Done 174034 tasks      | elapsed: 82.5min\n",
      "[Parallel(n_jobs=-1)]: Done 179984 tasks      | elapsed: 85.2min\n",
      "[Parallel(n_jobs=-1)]: Done 186034 tasks      | elapsed: 87.8min\n",
      "[Parallel(n_jobs=-1)]: Done 192184 tasks      | elapsed: 90.2min\n",
      "[Parallel(n_jobs=-1)]: Done 198434 tasks      | elapsed: 93.2min\n",
      "[Parallel(n_jobs=-1)]: Done 204784 tasks      | elapsed: 97.7min\n",
      "[Parallel(n_jobs=-1)]: Done 211234 tasks      | elapsed: 101.4min\n",
      "[Parallel(n_jobs=-1)]: Done 217784 tasks      | elapsed: 105.0min\n",
      "[Parallel(n_jobs=-1)]: Done 224434 tasks      | elapsed: 108.8min\n",
      "[Parallel(n_jobs=-1)]: Done 231184 tasks      | elapsed: 113.5min\n",
      "[Parallel(n_jobs=-1)]: Done 238034 tasks      | elapsed: 116.5min\n",
      "[Parallel(n_jobs=-1)]: Done 244984 tasks      | elapsed: 119.7min\n",
      "[Parallel(n_jobs=-1)]: Done 252034 tasks      | elapsed: 123.2min\n",
      "[Parallel(n_jobs=-1)]: Done 259184 tasks      | elapsed: 126.3min\n",
      "[Parallel(n_jobs=-1)]: Done 266434 tasks      | elapsed: 129.3min\n",
      "[Parallel(n_jobs=-1)]: Done 273784 tasks      | elapsed: 132.7min\n",
      "[Parallel(n_jobs=-1)]: Done 281234 tasks      | elapsed: 136.1min\n",
      "[Parallel(n_jobs=-1)]: Done 288784 tasks      | elapsed: 139.9min\n",
      "[Parallel(n_jobs=-1)]: Done 296434 tasks      | elapsed: 143.6min\n",
      "[Parallel(n_jobs=-1)]: Done 304184 tasks      | elapsed: 147.4min\n",
      "[Parallel(n_jobs=-1)]: Done 312034 tasks      | elapsed: 150.2min\n",
      "[Parallel(n_jobs=-1)]: Done 319984 tasks      | elapsed: 153.4min\n",
      "[Parallel(n_jobs=-1)]: Done 328034 tasks      | elapsed: 156.6min\n",
      "[Parallel(n_jobs=-1)]: Done 336184 tasks      | elapsed: 160.0min\n",
      "[Parallel(n_jobs=-1)]: Done 344434 tasks      | elapsed: 163.5min\n",
      "[Parallel(n_jobs=-1)]: Done 352784 tasks      | elapsed: 167.1min\n",
      "[Parallel(n_jobs=-1)]: Done 361234 tasks      | elapsed: 170.6min\n",
      "[Parallel(n_jobs=-1)]: Done 369784 tasks      | elapsed: 174.0min\n",
      "[Parallel(n_jobs=-1)]: Done 378434 tasks      | elapsed: 177.4min\n",
      "[Parallel(n_jobs=-1)]: Done 387184 tasks      | elapsed: 181.0min\n",
      "[Parallel(n_jobs=-1)]: Done 396034 tasks      | elapsed: 184.8min\n",
      "[Parallel(n_jobs=-1)]: Done 404984 tasks      | elapsed: 188.4min\n",
      "[Parallel(n_jobs=-1)]: Done 414034 tasks      | elapsed: 192.6min\n",
      "[Parallel(n_jobs=-1)]: Done 423184 tasks      | elapsed: 196.5min\n",
      "[Parallel(n_jobs=-1)]: Done 432434 tasks      | elapsed: 200.7min\n",
      "[Parallel(n_jobs=-1)]: Done 441784 tasks      | elapsed: 204.9min\n",
      "[Parallel(n_jobs=-1)]: Done 451234 tasks      | elapsed: 208.4min\n",
      "[Parallel(n_jobs=-1)]: Done 460784 tasks      | elapsed: 212.8min\n",
      "[Parallel(n_jobs=-1)]: Done 470434 tasks      | elapsed: 217.0min\n",
      "[Parallel(n_jobs=-1)]: Done 480184 tasks      | elapsed: 221.0min\n",
      "[Parallel(n_jobs=-1)]: Done 490034 tasks      | elapsed: 225.1min\n",
      "[Parallel(n_jobs=-1)]: Done 499984 tasks      | elapsed: 228.9min\n",
      "[Parallel(n_jobs=-1)]: Done 510034 tasks      | elapsed: 232.6min\n",
      "[Parallel(n_jobs=-1)]: Done 520184 tasks      | elapsed: 236.4min\n",
      "[Parallel(n_jobs=-1)]: Done 530434 tasks      | elapsed: 240.2min\n",
      "[Parallel(n_jobs=-1)]: Done 540784 tasks      | elapsed: 244.1min\n",
      "[Parallel(n_jobs=-1)]: Done 551234 tasks      | elapsed: 247.7min\n",
      "[Parallel(n_jobs=-1)]: Done 561784 tasks      | elapsed: 251.3min\n",
      "[Parallel(n_jobs=-1)]: Done 572434 tasks      | elapsed: 255.3min\n",
      "[Parallel(n_jobs=-1)]: Done 583184 tasks      | elapsed: 259.6min\n",
      "[Parallel(n_jobs=-1)]: Done 594034 tasks      | elapsed: 264.3min\n",
      "[Parallel(n_jobs=-1)]: Done 604984 tasks      | elapsed: 269.2min\n",
      "[Parallel(n_jobs=-1)]: Done 616034 tasks      | elapsed: 274.8min\n",
      "[Parallel(n_jobs=-1)]: Done 627184 tasks      | elapsed: 279.9min\n",
      "[Parallel(n_jobs=-1)]: Done 638434 tasks      | elapsed: 284.6min\n",
      "[Parallel(n_jobs=-1)]: Done 649784 tasks      | elapsed: 289.8min\n",
      "[Parallel(n_jobs=-1)]: Done 661234 tasks      | elapsed: 294.3min\n",
      "[Parallel(n_jobs=-1)]: Done 672784 tasks      | elapsed: 298.9min\n",
      "[Parallel(n_jobs=-1)]: Done 684434 tasks      | elapsed: 304.1min\n",
      "[Parallel(n_jobs=-1)]: Done 696184 tasks      | elapsed: 308.7min\n",
      "[Parallel(n_jobs=-1)]: Done 708034 tasks      | elapsed: 313.0min\n",
      "[Parallel(n_jobs=-1)]: Done 719984 tasks      | elapsed: 317.7min\n",
      "[Parallel(n_jobs=-1)]: Done 732034 tasks      | elapsed: 322.3min\n",
      "[Parallel(n_jobs=-1)]: Done 744184 tasks      | elapsed: 326.7min\n",
      "[Parallel(n_jobs=-1)]: Done 756434 tasks      | elapsed: 331.4min\n",
      "[Parallel(n_jobs=-1)]: Done 768784 tasks      | elapsed: 336.0min\n",
      "[Parallel(n_jobs=-1)]: Done 781234 tasks      | elapsed: 367.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 793784 tasks      | elapsed: 372.3min\n",
      "[Parallel(n_jobs=-1)]: Done 806434 tasks      | elapsed: 377.3min\n",
      "[Parallel(n_jobs=-1)]: Done 819184 tasks      | elapsed: 382.7min\n",
      "[Parallel(n_jobs=-1)]: Done 832034 tasks      | elapsed: 388.1min\n",
      "[Parallel(n_jobs=-1)]: Done 844984 tasks      | elapsed: 393.3min\n",
      "[Parallel(n_jobs=-1)]: Done 858034 tasks      | elapsed: 398.2min\n",
      "[Parallel(n_jobs=-1)]: Done 871184 tasks      | elapsed: 402.8min\n",
      "[Parallel(n_jobs=-1)]: Done 884434 tasks      | elapsed: 407.5min\n",
      "[Parallel(n_jobs=-1)]: Done 897784 tasks      | elapsed: 412.4min\n",
      "[Parallel(n_jobs=-1)]: Done 911234 tasks      | elapsed: 417.5min\n",
      "[Parallel(n_jobs=-1)]: Done 911250 out of 911250 | elapsed: 417.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================================\n",
      "('The best Parameter: ', LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "        importance_type='split', learning_rate=0.3, max_bins=5,\n",
      "        max_depth=5, max_features=10, min_child_samples=12,\n",
      "        min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,\n",
      "        n_jobs=-1, num_iteration=50, num_leaves=31, objective=None,\n",
      "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0))\n",
      "('The best parameter :', {'learning_rate': 0.3, 'max_bins': 5, 'n_estimators': 10, 'num_iteration': 50, 'max_features': 10, 'min_child_samples': 12, 'max_depth': 5})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.1s remaining:    0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cv_results', array([0.81481481, 0.77777778, 0.80246914, 0.8625    , 0.79746835]))\n",
      "()\n",
      "{'num_leaves': 31, 'reg_alpha': 0.0, 'n_jobs': -1, 'learning_rate': 0.3, 'n_estimators': 10, 'num_iteration': 50, 'subsample_for_bin': 200000, 'max_features': 10, 'importance_type': 'split', 'boosting_type': 'gbdt', 'colsample_bytree': 1.0, 'silent': True, 'subsample_freq': 0, 'min_child_samples': 12, 'max_bins': 5, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'subsample': 1.0, 'reg_lambda': 0.0, 'random_state': None, 'objective': None, 'class_weight': None, 'max_depth': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation using the best fit model\n",
    "\n",
    "\n",
    "\n",
    "def cross_validation_and_fitting(model, X_train, y_train):\n",
    "    model = training_model(X_train, y_train)\n",
    "        \n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv = kfold, scoring = 'accuracy', n_jobs = -1, verbose = 1)\n",
    "    print('cv_results', cv_results)\n",
    "   \n",
    "    #Final fitting of the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    print(); print(model.get_params(deep = True))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = cross_validation_and_fitting(model, X_train, y_train)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy of the model is :', 0.7029702970297029)\n",
      "('Balanced Accuracy of the model is :', 0.5754310344827587)\n",
      "('Area under curve and ROC :', 0.5754310344827587)\n",
      "('Confusion Matrix :', array([[63,  9],\n",
      "       [21,  8]], dtype=int64))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.88      0.81        72\n",
      "           1       0.47      0.28      0.35        29\n",
      "\n",
      "   micro avg       0.70      0.70      0.70       101\n",
      "   macro avg       0.61      0.58      0.58       101\n",
      "weighted avg       0.67      0.70      0.68       101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained model on test dataset\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    balancedaccuracy = balanced_accuracy_score(y_test, pred)\n",
    "    rocauc = roc_auc_score(y_test, pred)\n",
    "    confmat = confusion_matrix(y_test, pred)\n",
    "    classrprt = classification_report(y_test, pred)\n",
    "    \n",
    "    print(\"Accuracy of the model is :\", accuracy)\n",
    "    print(\"Balanced Accuracy of the model is :\", balancedaccuracy)\n",
    "    print(\"Area under curve and ROC :\", rocauc)\n",
    "  #  print(\"F1 Score of the model is :\", fonescore)\n",
    "    print(\"Confusion Matrix :\", confmat)\n",
    "    print(classrprt)\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "model = evaluate_model(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 50)\n",
    "\n",
    "classifier = clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = RandomForestClassifier(n_jobs = -1, n_estimators = 500)\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   23.5s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   26.5s finished\n",
      "C:\\Users\\sjha\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:842: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best estimators', RandomForestClassifier(bootstrap='True', class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features=5, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=3, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=329, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))\n",
      "('Best Score', 0.7761194029850746)\n"
     ]
    }
   ],
   "source": [
    "def training_modelrf(X_train, y_train):\n",
    "    est = RandomForestClassifier()\n",
    "    #Grid Search\n",
    "\n",
    "    \n",
    "    rf_parameters = {\n",
    "    'max_depth'      : sp_randInt(3, 12),\n",
    "    'n_estimators'   : sp_randInt(100, 1000),\n",
    "    'max_features'   : sp_randInt(1, 10),\n",
    "    'criterion'      : ['gini', 'entropy'],\n",
    "    'bootstrap'      : ['True', 'False'],\n",
    "    'min_samples_leaf': sp_randInt(1,4)\n",
    "         \n",
    "    }\n",
    "    \n",
    "    randmrf = RandomizedSearchCV(estimator = est, param_distributions = rf_parameters, cv = kfold, n_iter = 10, verbose = 1, n_jobs = -1, random_state = 50)\n",
    "    randmrf.fit(X_train, y_train)\n",
    "    \n",
    "    #Results from random search\n",
    "    print(\"Best estimators\", randmrf.best_estimator_)\n",
    "    print(\"Best Score\", randmrf.best_score_)\n",
    "    \n",
    "    return randmrf.best_estimator_\n",
    "    \n",
    "#    return grid.best_estimator_    \n",
    "\n",
    "model = training_modelrf(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.2s remaining:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cv_results', array([0.80246914, 0.81481481, 0.75308642, 0.775     , 0.73417722]))\n",
      "()\n",
      "{'warm_start': False, 'oob_score': False, 'n_jobs': None, 'min_impurity_decrease': 0.0, 'verbose': 0, 'max_leaf_nodes': None, 'bootstrap': 'True', 'min_samples_leaf': 3, 'n_estimators': 329, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'criterion': 'gini', 'random_state': None, 'min_impurity_split': None, 'max_features': 5, 'max_depth': 4, 'class_weight': None}\n"
     ]
    }
   ],
   "source": [
    " #Cross Validation using the best fit model\n",
    "\n",
    "\n",
    "\n",
    "def cross_validation_and_fitting(model, X_train, y_train):\n",
    " #   model = training_model(X_train, y_train)\n",
    "        \n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv = 5, scoring = 'accuracy', n_jobs = -1, verbose = 1)\n",
    "    print('cv_results', cv_results)\n",
    "   \n",
    "    #Final fitting of the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    print(); print(model.get_params(deep = True))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = cross_validation_and_fitting(model, X_train, y_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy of the model is :', 0.8217821782178217)\n",
      "('Balanced Accuracy of the model is :', 0.711038961038961)\n",
      "('Area under curve and ROC :', 0.7110389610389609)\n",
      "('Confusion Matrix :', array([[71,  6],\n",
      "       [12, 12]], dtype=int64))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.92      0.89        77\n",
      "           1       0.67      0.50      0.57        24\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       101\n",
      "   macro avg       0.76      0.71      0.73       101\n",
      "weighted avg       0.81      0.82      0.81       101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    balancedaccuracy = balanced_accuracy_score(y_test, pred)\n",
    "    rocauc = roc_auc_score(y_test, pred)\n",
    "    confmat = confusion_matrix(y_test, pred)\n",
    "    classrprt = classification_report(y_test, pred)\n",
    "    \n",
    "    print(\"Accuracy of the model is :\", accuracy)\n",
    "    print(\"Balanced Accuracy of the model is :\", balancedaccuracy)\n",
    "    print(\"Area under curve and ROC :\", rocauc)\n",
    "  #  print(\"F1 Score of the model is :\", fonescore)\n",
    "    print(\"Confusion Matrix :\", confmat)\n",
    "    print(classrprt)\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "model = evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named tpot",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-70f222d22a16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtpot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named tpot"
     ]
    }
   ],
   "source": [
    "import tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  1.2min finished\n",
      "C:\\Users\\sjha\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:842: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best estimators', RandomForestClassifier(bootstrap='True', class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features=5, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=3, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=329, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))\n",
      "('Best Score', 0.7761194029850746)\n"
     ]
    }
   ],
   "source": [
    "def training_modelrf(X_train, y_train):\n",
    "    est = RandomForestClassifier()\n",
    "    #Grid Search\n",
    "\n",
    "    \n",
    "    rf_parameters = {\n",
    "    'max_depth'      : sp_randInt(3, 12),\n",
    "    'n_estimators'   : sp_randInt(100, 1000),\n",
    "    'max_features'   : sp_randInt(1, 10),\n",
    "    'criterion'      : ['gini', 'entropy'],\n",
    "    'bootstrap'      : ['True', 'False'],\n",
    "    'min_samples_leaf': sp_randInt(1,4)\n",
    "         \n",
    "    }\n",
    "    \n",
    "    randmrf = RandomizedSearchCV(estimator = est, param_distributions = rf_parameters, cv = kfold, n_iter = 10, verbose = 1, n_jobs = -1, random_state = 50)\n",
    "    randmrf.fit(X_train, y_train)\n",
    "    \n",
    "    #Results from random search\n",
    "    print(\"Best estimators\", randmrf.best_estimator_)\n",
    "    print(\"Best Score\", randmrf.best_score_)\n",
    "    \n",
    "    return randmrf.best_estimator_\n",
    "    \n",
    "#    return grid.best_estimator_    \n",
    "\n",
    "model = training_modelrf(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
